# Robots.txt Configuration
# This file should contain:
# - Search engine crawling instructions
# - Disallowed paths for bots
# - Sitemap location reference
# - Crawl delay specifications
# 
# Production configuration:
# User-agent: *
# Allow: /
# Disallow: /admin/
# Disallow: /api/
# Disallow: /_nuxt/
# Sitemap: https://yourdomain.com/sitemap.xml
# 
# Development configuration (current):
User-agent: *
Disallow: /
